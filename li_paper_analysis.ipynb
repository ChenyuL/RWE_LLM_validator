{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Li-Paper Analysis: Agreement Between LLM Extractors and Validators\n",
    "\n",
    "This notebook analyzes the agreement between different LLM configurations in evaluating papers against the Li-Paper checklist:\n",
    "\n",
    "1. OpenAI as extractor, Claude as validator\n",
    "2. Claude as extractor, OpenAI as validator\n",
    "3. Comparison between model outputs\n",
    "\n",
    "The analysis includes visualizations and statistical tests to quantify the level of agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "First, we'll load the JSON files for all papers with Li-Paper evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Directory containing the paper results\n",
    "base_dir = 'output/paper_results'\n",
    "\n",
    "# Get all directories with Li-Paper suffix\n",
    "li_paper_dirs = [d for d in os.listdir(base_dir) if d.endswith('_Li-Paper') and os.path.isdir(os.path.join(base_dir, d))]\n",
    "print(f\"Found {len(li_paper_dirs)} Li-Paper directories\")\n",
    "print(li_paper_dirs[:5], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to extract data from a report file\n",
    "def extract_report_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract paper ID\n",
    "        paper_id = data.get('paper', '').replace('.pdf', '')\n",
    "        \n",
    "        # Extract validation summary\n",
    "        summary = data.get('validation_summary', {})\n",
    "        \n",
    "        # Extract model information if available\n",
    "        model_info = data.get('model_info', {})\n",
    "        if not model_info and 'model_info' in summary:\n",
    "            model_info = summary.get('model_info', {})\n",
    "        \n",
    "        # Extract items data\n",
    "        items = data.get('items', {})\n",
    "        \n",
    "        # Determine configuration based on filename\n",
    "        if 'openai_claude' in file_path:\n",
    "            config = 'openai_claude'\n",
    "        elif 'claude_openai' in file_path:\n",
    "            config = 'claude_openai'\n",
    "        else:\n",
    "            config = 'unknown'\n",
    "        \n",
    "        return {\n",
    "            'paper_id': paper_id,\n",
    "            'config': config,\n",
    "            'summary': summary,\n",
    "            'model_info': model_info,\n",
    "            'items': items,\n",
    "            'file_path': file_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load all report data\n",
    "all_reports = []\n",
    "\n",
    "for paper_dir in li_paper_dirs:\n",
    "    dir_path = os.path.join(base_dir, paper_dir)\n",
    "    for file in os.listdir(dir_path):\n",
    "        if ('openai_claude_report' in file or 'claude_openai_report' in file) and file.endswith('.json'):\n",
    "            file_path = os.path.join(dir_path, file)\n",
    "            report_data = extract_report_data(file_path)\n",
    "            if report_data:\n",
    "                all_reports.append(report_data)\n",
    "\n",
    "print(f\"Loaded {len(all_reports)} reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame with summary information\n",
    "summary_data = []\n",
    "\n",
    "for report in all_reports:\n",
    "    summary = report['summary']\n",
    "    summary_data.append({\n",
    "        'paper_id': report['paper_id'],\n",
    "        'config': report['config'],\n",
    "        'total_items': summary.get('total_items', 0),\n",
    "        'agree_with_extractor': summary.get('agree_with_extractor', 0),\n",
    "        'disagree_with_extractor': summary.get('disagree_with_extractor', 0),\n",
    "        'unknown': summary.get('unknown', 0),\n",
    "        'agreement_rate': summary.get('agreement_rate', 0),\n",
    "        'extractor': report['model_info'].get('extractor', ''),\n",
    "        'validator': report['model_info'].get('validator', '')\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Agreement Rates by Configuration\n",
    "\n",
    "First, let's analyze the agreement rates for each configuration (OpenAI as extractor vs. Claude as validator, and vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate average agreement rates by configuration\n",
    "avg_agreement = summary_df.groupby('config')['agreement_rate'].agg(['mean', 'std', 'count']).reset_index()\n",
    "avg_agreement.columns = ['Configuration', 'Mean Agreement Rate (%)', 'Std Dev', 'Count']\n",
    "avg_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize agreement rates by configuration\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar plot\n",
    "ax = sns.barplot(x='Configuration', y='Mean Agreement Rate (%)', data=avg_agreement, palette='viridis')\n",
    "\n",
    "# Add error bars\n",
    "for i, row in avg_agreement.iterrows():\n",
    "    ax.errorbar(i, row['Mean Agreement Rate (%)'], yerr=row['Std Dev'], color='black', capsize=10, linewidth=2)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(avg_agreement['Mean Agreement Rate (%)']):\n",
    "    ax.text(i, v + 1, f\"{v:.2f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.title('Mean Agreement Rate by Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Configuration', fontsize=14)\n",
    "plt.ylim(0, 105)  # Set y-axis limit to accommodate error bars and labels\n",
    "plt.tight_layout()\n",
    "plt.savefig('agreement_by_config.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Distribution of Agreement Rates\n",
    "\n",
    "Let's examine the distribution of agreement rates across all papers for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a violin plot to show the distribution of agreement rates\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='config', y='agreement_rate', data=summary_df, palette='viridis', inner='box')\n",
    "ax = sns.swarmplot(x='config', y='agreement_rate', data=summary_df, color='white', edgecolor='black', size=8, alpha=0.7)\n",
    "\n",
    "plt.title('Distribution of Agreement Rates by Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Configuration', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('agreement_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Statistical Comparison of Configurations\n",
    "\n",
    "Let's perform a statistical test to determine if there's a significant difference in agreement rates between the two configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split data by configuration\n",
    "openai_claude = summary_df[summary_df['config'] == 'openai_claude']['agreement_rate']\n",
    "claude_openai = summary_df[summary_df['config'] == 'claude_openai']['agreement_rate']\n",
    "\n",
    "# Perform Mann-Whitney U test (non-parametric test for independent samples)\n",
    "u_stat, p_value = stats.mannwhitneyu(openai_claude, claude_openai, alternative='two-sided')\n",
    "\n",
    "print(f\"Mann-Whitney U test results:\")\n",
    "print(f\"U statistic: {u_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Significant difference at Î±=0.05: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 4: Agreement on Individual Checklist Items\n",
    "\n",
    "Now, let's analyze the agreement on individual checklist items across all papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract item-level data\n",
    "item_data = []\n",
    "\n",
    "for report in all_reports:\n",
    "    for item_id, item in report['items'].items():\n",
    "        item_data.append({\n",
    "            'paper_id': report['paper_id'],\n",
    "            'config': report['config'],\n",
    "            'item_id': item_id,\n",
    "            'compliance': item.get('compliance', ''),\n",
    "            'correct_answer': item.get('correct_answer', ''),\n",
    "            'description': item.get('description', '')\n",
    "        })\n",
    "\n",
    "item_df = pd.DataFrame(item_data)\n",
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate agreement rate by checklist item\n",
    "item_agreement = item_df.groupby(['item_id', 'config'])['compliance'].apply(\n",
    "    lambda x: (x == 'agree with extractor').mean() * 100\n",
    ").reset_index()\n",
    "item_agreement.columns = ['Item ID', 'Configuration', 'Agreement Rate (%)']\n",
    "\n",
    "# Pivot the data for easier comparison\n",
    "item_agreement_pivot = item_agreement.pivot(index='Item ID', columns='Configuration', values='Agreement Rate (%)')\n",
    "item_agreement_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Sort by item ID numerically\n",
    "item_agreement_pivot['Item ID'] = pd.to_numeric(item_agreement_pivot['Item ID'])\n",
    "item_agreement_pivot.sort_values('Item ID', inplace=True)\n",
    "\n",
    "item_agreement_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize agreement rates by checklist item\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Reshape data for plotting\n",
    "plot_data = pd.melt(item_agreement_pivot, id_vars=['Item ID'], \n",
    "                    value_vars=['openai_claude', 'claude_openai'],\n",
    "                    var_name='Configuration', value_name='Agreement Rate (%)')\n",
    "\n",
    "# Create the grouped bar chart\n",
    "ax = sns.barplot(x='Item ID', y='Agreement Rate (%)', hue='Configuration', data=plot_data, palette='viridis')\n",
    "\n",
    "plt.title('Agreement Rate by Checklist Item and Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Checklist Item ID', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 105)\n",
    "plt.legend(title='Configuration', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('agreement_by_item.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 5: Model Output Comparison\n",
    "\n",
    "Now, let's compare the model outputs (correct_answer field) between OpenAI and Claude to see if they agree with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a dictionary to store model outputs by paper and item\n",
    "model_outputs = {}\n",
    "\n",
    "for report in all_reports:\n",
    "    paper_id = report['paper_id']\n",
    "    config = report['config']\n",
    "    \n",
    "    if paper_id not in model_outputs:\n",
    "        model_outputs[paper_id] = {}\n",
    "    \n",
    "    for item_id, item in report['items'].items():\n",
    "        if item_id not in model_outputs[paper_id]:\n",
    "            model_outputs[paper_id][item_id] = {}\n",
    "        \n",
    "        model_outputs[paper_id][item_id][config] = item.get('correct_answer', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to compare model outputs and determine if they agree\n",
    "def outputs_agree(output1, output2):\n",
    "    # Simple string comparison for now\n",
    "    # Could be enhanced with semantic similarity or other NLP techniques\n",
    "    if not output1 or not output2:\n",
    "        return False\n",
    "    \n",
    "    # Check for exact match\n",
    "    if output1 == output2:\n",
    "        return True\n",
    "    \n",
    "    # Check for 'unknown' or similar values\n",
    "    unknown_patterns = ['unknown', 'not enough information', 'cannot determine']\n",
    "    if any(pattern in output1.lower() for pattern in unknown_patterns) and \\\n",
    "       any(pattern in output2.lower() for pattern in unknown_patterns):\n",
    "        return True\n",
    "    \n",
    "    # Check for yes/no agreement\n",
    "    yes_patterns = ['yes', 'complies', 'compliant', 'fulfilled']\n",
    "    no_patterns = ['no', 'does not comply', 'non-compliant', 'not fulfilled']\n",
    "    \n",
    "    output1_yes = any(pattern in output1.lower() for pattern in yes_patterns)\n",
    "    output1_no = any(pattern in output1.lower() for pattern in no_patterns)\n",
    "    output2_yes = any(pattern in output2.lower() for pattern in yes_patterns)\n",
    "    output2_no = any(pattern in output2.lower() for pattern in no_patterns)\n",
    "    \n",
    "    if (output1_yes and output2_yes) or (output1_no and output2_no):\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement\n",
    "output_agreement_data = []\n",
    "\n",
    "for paper_id, items in model_outputs.items():\n",
    "    for item_id, configs in items.items():\n",
    "        if 'openai_claude' in configs and 'claude_openai' in configs:\n",
    "            openai_claude_output = configs['openai_claude']\n",
    "            claude_openai_output = configs['claude_openai']\n",
    "            \n",
    "            agreement = outputs_agree(openai_claude_output, claude_openai_output)\n",
    "            \n",
    "            output_agreement_data.append({\n",
    "                'paper_id': paper_id,\n",
    "                'item_id': item_id,\n",
    "                'openai_claude_output': openai_claude_output,\n",
    "                'claude_openai_output': claude_openai_output,\n",
    "                'models_agree': agreement\n",
    "            })\n",
    "\n",
    "output_agreement_df = pd.DataFrame(output_agreement_data)\n",
    "output_agreement_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate overall model output agreement rate\n",
    "overall_output_agreement = output_agreement_df['models_agree'].mean() * 100\n",
    "print(f\"Overall model output agreement rate: {overall_output_agreement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement by paper\n",
    "paper_output_agreement = output_agreement_df.groupby('paper_id')['models_agree'].mean() * 100\n",
    "paper_output_agreement = paper_output_agreement.reset_index()\n",
    "paper_output_agreement.columns = ['Paper ID', 'Model Output Agreement Rate (%)']\n",
    "paper_output_agreement.sort_values('Model Output Agreement Rate (%)', ascending=False, inplace=True)\n",
    "paper_output_agreement.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model output agreement by paper\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x='Paper ID', y='Model Output Agreement Rate (%)', data=paper_output_agreement, palette='viridis')\n",
    "\n",
    "plt.title('Model Output Agreement Rate by Paper', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Paper ID', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_output_agreement_by_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement by checklist item\n",
    "item_output_agreement = output_agreement_df.groupby('item_id')['models_agree'].mean() * 100\n",
    "item_output_agreement = item_output_agreement.reset_index()\n",
    "item_output_agreement.columns = ['Item ID', 'Model Output Agreement Rate (%)']\n",
    "\n",
    "# Sort by item ID numerically\n",
    "item_output_agreement['Item ID'] = pd.to_numeric(item_output_agreement['Item ID'])\n",
    "item_output_agreement.sort_values('Item ID', inplace=True)\n",
    "\n",
    "item_output_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model output agreement by checklist item\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x='Item ID', y='Model Output Agreement Rate (%)', data=item_output_agreement, palette='viridis')\n",
    "\n",
    "plt.title('Model Output Agreement Rate by Checklist Item', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Checklist Item ID', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_output_agreement_by_item.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 6: Correlation Between Validator Agreement and Model Output Agreement\n",
    "\n",
    "Let's examine if there's a correlation between validator agreement rates and model output agreement rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate agreement rates by paper for both configurations\n",
    "paper_agreement = summary_df.groupby(['paper_id', 'config'])['agreement_rate'].mean().reset_index()\n",
    "paper_agreement_pivot = paper_agreement.pivot(index='paper_id', columns='config', values='agreement_rate').reset_index()\n",
    "paper_agreement_pivot.columns = ['paper_id', 'claude_openai_agreement', 'openai_claude_agreement']\n",
    "\n",
    "# Merge with model output agreement data\n",
    "merged_agreement = pd.merge(paper_agreement_pivot, paper_output_agreement, left_on='paper_id', right_on='Paper ID', how='inner')\n",
    "merged_agreement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation coefficients\n",
    "corr_openai_claude = merged_agreement['openai_claude_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "corr_claude_openai = merged_agreement['claude_openai_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "\n",
    "print(f\"Correlation between OpenAI-Claude validator agreement and model output agreement: {corr_openai_claude:.4f}\")\n",
    "print(f\"Correlation between Claude-OpenAI validator agreement and model output agreement: {corr_claude_openai:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create scatter plots to visualize correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot for OpenAI-Claude configuration\n",
    "sns.regplot(x='openai_claude_agreement', y='Model Output Agreement Rate (%)', \n",
    "            data=merged_agreement, ax=axes[0], scatter_kws={'alpha':0.7, 's':100}, line_kws={'color':'red'})\n",
    "axes[0].set_title(f'OpenAI-Claude Validator Agreement vs. Model Output Agreement\\nCorrelation: {corr_openai_claude:.4f}', fontsize=14)\n",
    "axes[0].set_xlabel('OpenAI-Claude Validator Agreement Rate (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Model Output Agreement Rate (%)', fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for Claude-OpenAI configuration\n",
    "sns.regplot(x='claude_openai_agreement', y='Model Output Agreement Rate (%)', \n",
    "            data=merged_agreement, ax=axes[1], scatter_kws={'alpha':0.7, 's':100}, line_kws={'color':'red'})\n",
    "axes[1].set_title(f'Claude-OpenAI Validator Agreement vs. Model Output Agreement\\nCorrelation: {corr_claude_openai:.4f}', fontsize=14)\n",
    "axes[1].set_xlabel('Claude-OpenAI Validator Agreement Rate (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Model Output Agreement Rate (%)', fontsize=12)\n",
    "axes[1].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This analysis has examined the agreement between different LLM configurations in evaluating papers against the Li-Paper checklist. The key findings include:\n",
    "\n",
    "1. **Agreement Rates**: We observed high agreement rates between extractors and validators across both configurations, with some variation between papers and checklist items.\n",
    "\n",
    "2. **Configuration Comparison**: The statistical comparison showed whether there is a significant difference in agreement rates between the OpenAI-Claude and Claude-OpenAI configurations.\n",
    "\n",
    "3. **Model Output Agreement**: The analysis of model outputs revealed the extent to which OpenAI and Claude produce similar assessments when evaluating the same papers against the Li-Paper checklist.\n",
    "\n",
    "4. **Correlation Analysis**: We examined the relationship between validator agreement rates and model output agreement rates, providing insights into how these metrics relate to each other.\n",
    "\n",
    "These findings contribute to our understanding of how different LLM configurations perform in the context of evaluating research papers against reporting guidelines, and can inform future work in this area."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
