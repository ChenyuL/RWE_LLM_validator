# Li-Paper Validation and Analysis Framework

This project implements an automated validation framework for evaluating research papers against the Li-Paper checklist using Large Language Models (LLMs). The framework uses a combination of OpenAI and Claude models to extract information from papers and validate compliance with reporting guidelines.

## Overview

The framework consists of three main components:

1. **Reasoner**: Processes the Li-Paper guidelines and generates prompts for extraction
2. **Extractor**: Extracts information from research papers based on the prompts
3. **Validator**: Validates the extracted information against the guidelines

Two configurations are implemented:
- **OpenAI as extractor, Claude as validator**
- **Claude as extractor, OpenAI as validator**

The results from both configurations are analyzed to measure agreement rates and identify patterns in model performance.

## Directory Structure

- `data/Papers/`: Contains the research papers in PDF format
- `data/Guidelines/Li-Paper/`: Contains the Li-Paper checklist guidelines
- `output/`: Contains the output files from the validation process
- `output/paper_results/`: Contains the validation results for each paper
- `output/prompts/`: Contains the prompts generated by the reasoner
- `src/`: Contains the source code for the framework
- `scripts/`: Contains utility scripts for analysis and visualization

## Running the Framework

### Prerequisites

- Python 3.8+
- Required Python packages (install with `pip install -r requirements.txt`)
- API keys for OpenAI and Anthropic (Claude) in a `.env` file

### Validation Process

The entire validation and analysis process can be run using the provided shell script:

```bash
chmod +x run_li_paper_validation_and_analysis.sh
./run_li_paper_validation_and_analysis.sh
```

This script will:
1. Process each paper in the `fixed_sampled_papers.json` list
2. Run OpenAI as extractor and Claude as validator for each paper
3. Run Claude as extractor and OpenAI as validator for each paper
4. Run the analysis script to generate visualizations and statistics
5. Clean up temporary files

Alternatively, you can run each step separately using the dedicated scripts:

1. **Run only OpenAI as extractor and Claude as validator**:
   ```bash
   ./run_openai_claude_validation.sh
   ```

2. **Run only Claude as extractor and OpenAI as validator**:
   ```bash
   ./run_claude_openai_validation.sh
   ```

3. **Run only the analysis after validation is complete**:
   ```bash
   ./run_analysis_only.sh
   ```

These separate scripts are useful if you want to run only specific parts of the process or if you need to restart a specific step.

### Manual Execution

If you prefer to run the steps manually:

1. **Run OpenAI as extractor and Claude as validator**:
   ```bash
   python test_record_validation_openai_claude_fixed.py --mode extractor --prompts "output/prompts/20250319_123039_openai_reasoner_Li-Paper_prompts.json" --paper "data/Papers/PAPER_ID.pdf" --checklist "Li-Paper"
   ```

2. **Run Claude as extractor and OpenAI as validator**:
   ```bash
   python test_record_validation_claude_openai_fixed.py --mode extractor --prompts "output/prompts/20250319_123039_openai_reasoner_Li-Paper_prompts.json" --paper "data/Papers/PAPER_ID.pdf" --checklist "Li-Paper"
   ```

3. **Run the analysis script**:
   ```bash
   python run_li_paper_analysis.py
   ```

## Analysis Results

The analysis script generates several visualizations and statistics:

1. **Agreement Rates by Configuration**: Compares the agreement rates between OpenAI-Claude and Claude-OpenAI configurations
2. **Distribution of Agreement Rates**: Shows the distribution of agreement rates across papers
3. **Agreement on Individual Checklist Items**: Analyzes agreement rates for each checklist item
4. **Model Output Comparison**: Compares the actual outputs from OpenAI and Claude models
5. **Correlation Analysis**: Examines the correlation between validator agreement and model output agreement

The results are saved as PNG files in the current directory, and a summary report is generated in `li_paper_analysis_summary.md`.

## Key Findings

The analysis reveals several interesting patterns:

1. **Configuration Difference**: The OpenAI-Claude configuration shows significantly higher agreement rates than the Claude-OpenAI configuration.
2. **Low Model Output Agreement**: Despite high validator agreement rates, the direct model output agreement is relatively low, suggesting that the models often reach different conclusions when evaluating papers independently.
3. **Negative Correlation**: There is a negative correlation between validator agreement and model output agreement, which warrants further investigation.

For a detailed analysis of the results, refer to the `li_paper_analysis_summary.md` file.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
