{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-based Li-Paper Analysis Notebook\n",
    "\n",
    "This notebook analyzes the results of the RAG-based Li-Paper checklist validation process, comparing the OpenAI-Claude and Claude-OpenAI configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Starting RAG-based Li-Paper analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Directory containing the RAG paper results\n",
    "base_dir = 'output/reports_rag'\n",
    "\n",
    "# Function to extract data from a report file\n",
    "def extract_report_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract paper ID\n",
    "        paper_id = data.get('paper', '').replace('.pdf', '')\n",
    "        \n",
    "        # Extract validation summary\n",
    "        summary = data.get('validation_summary', {})\n",
    "        \n",
    "        # Extract model information if available\n",
    "        model_info = data.get('model_info', {})\n",
    "        if not model_info and 'model_info' in summary:\n",
    "            model_info = summary.get('model_info', {})\n",
    "        \n",
    "        # Extract items data\n",
    "        items = data.get('items', {})\n",
    "        \n",
    "        # Determine configuration based on model info\n",
    "        extractor = model_info.get('extractor', '')\n",
    "        validator = model_info.get('validator', '')\n",
    "        \n",
    "        if 'openai' in extractor.lower() and 'claude' in validator.lower():\n",
    "            config = 'openai_claude'\n",
    "        elif 'claude' in extractor.lower() and 'openai' in validator.lower():\n",
    "            config = 'claude_openai'\n",
    "        else:\n",
    "            config = 'unknown'\n",
    "        \n",
    "        return {\n",
    "            'paper_id': paper_id,\n",
    "            'config': config,\n",
    "            'summary': summary,\n",
    "            'model_info': model_info,\n",
    "            'items': items,\n",
    "            'file_path': file_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all report data\n",
    "all_reports = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"Error: Directory {base_dir} does not exist\")\n",
    "else:\n",
    "    # List all files in the directory\n",
    "    print(f\"Scanning directory: {base_dir}\")\n",
    "    files = os.listdir(base_dir)\n",
    "    report_files = [f for f in files if ('report' in f) and f.endswith('.json') and '_Li-Paper' in f]\n",
    "    print(f\"Found {len(report_files)} report files\")\n",
    "    \n",
    "    # Process each report file\n",
    "    for file in report_files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        report_data = extract_report_data(file_path)\n",
    "        if report_data:\n",
    "            all_reports.append(report_data)\n",
    "    \n",
    "    print(f\"Loaded {len(all_reports)} reports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame with summary information\n",
    "summary_data = []\n",
    "\n",
    "for report in all_reports:\n",
    "    summary = report['summary']\n",
    "    summary_data.append({\n",
    "        'paper_id': report['paper_id'],\n",
    "        'config': report['config'],\n",
    "        'total_items': summary.get('total_items', 0),\n",
    "        'agree_with_extractor': summary.get('agree_with_extractor', 0),\n",
    "        'disagree_with_extractor': summary.get('disagree_with_extractor', 0),\n",
    "        'unknown': summary.get('unknown', 0),\n",
    "        'agreement_rate': summary.get('agreement_rate', 0),\n",
    "        'extractor': report['model_info'].get('extractor', ''),\n",
    "        'validator': report['model_info'].get('validator', '')\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary DataFrame created with shape:\", summary_df.shape)\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate average agreement rates by configuration\n",
    "avg_agreement = summary_df.groupby('config')['agreement_rate'].agg(['mean', 'std', 'count']).reset_index()\n",
    "avg_agreement.columns = ['Configuration', 'Mean Agreement Rate (%)', 'Std Dev', 'Count']\n",
    "avg_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform Mann-Whitney U test (non-parametric test for independent samples)\n",
    "openai_claude = summary_df[summary_df['config'] == 'openai_claude']['agreement_rate']\n",
    "claude_openai = summary_df[summary_df['config'] == 'claude_openai']['agreement_rate']\n",
    "\n",
    "u_stat, p_value = stats.mannwhitneyu(openai_claude, claude_openai, alternative='two-sided')\n",
    "\n",
    "print(f\"Mann-Whitney U test results:\")\n",
    "print(f\"U statistic: {u_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Significant difference at Î±=0.05: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract model outputs\n",
    "model_outputs = {}\n",
    "\n",
    "for report in all_reports:\n",
    "    paper_id = report['paper_id']\n",
    "    config = report['config']\n",
    "    \n",
    "    if paper_id not in model_outputs:\n",
    "        model_outputs[paper_id] = {}\n",
    "    \n",
    "    for item_id, item in report['items'].items():\n",
    "        if item_id not in model_outputs[paper_id]:\n",
    "            model_outputs[paper_id][item_id] = {}\n",
    "        \n",
    "        model_outputs[paper_id][item_id][config] = item.get('correct_answer', '')\n",
    "\n",
    "# Function to compare model outputs and determine if they agree\n",
    "def outputs_agree(output1, output2):\n",
    "    # Simple string comparison for now\n",
    "    if not output1 or not output2:\n",
    "        return False\n",
    "    \n",
    "    # Check for exact match\n",
    "    if output1 == output2:\n",
    "        return True\n",
    "    \n",
    "    # Check for 'unknown' or similar values\n",
    "    unknown_patterns = ['unknown', 'not enough information', 'cannot determine']\n",
    "    if any(pattern in output1.lower() for pattern in unknown_patterns) and \\\n",
    "       any(pattern in output2.lower() for pattern in unknown_patterns):\n",
    "        return True\n",
    "    \n",
    "    # Check for yes/no agreement\n",
    "    yes_patterns = ['yes', 'complies', 'compliant', 'fulfilled']\n",
    "    no_patterns = ['no', 'does not comply', 'non-compliant', 'not fulfilled']\n",
    "    \n",
    "    output1_yes = any(pattern in output1.lower() for pattern in yes_patterns)\n",
    "    output1_no = any(pattern in output1.lower() for pattern in no_patterns)\n",
    "    output2_yes = any(pattern in output2.lower() for pattern in yes_patterns)\n",
    "    output2_no = any(pattern in output2.lower() for pattern in no_patterns)\n",
    "    \n",
    "    if (output1_yes and output2_yes) or (output1_no and output2_no):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Calculate model output agreement\n",
    "output_agreement_data = []\n",
    "\n",
    "for paper_id, items in model_outputs.items():\n",
    "    for item_id, configs in items.items():\n",
    "        if 'openai_claude' in configs and 'claude_openai' in configs:\n",
    "            openai_claude_output = configs['openai_claude']\n",
    "            claude_openai_output = configs['claude_openai']\n",
    "            \n",
    "            agreement = outputs_agree(openai_claude_output, claude_openai_output)\n",
    "            \n",
    "            output_agreement_data.append({\n",
    "                'paper_id': paper_id,\n",
    "                'item_id': item_id,\n",
    "                'openai_claude_output': openai_claude_output,\n",
    "                'claude_openai_output': claude_openai_output,\n",
    "                'models_agree': agreement\n",
    "            })\n",
    "\n",
    "output_agreement_df = pd.DataFrame(output_agreement_data)\n",
    "overall_output_agreement = output_agreement_df['models_agree'].mean() * 100\n",
    "print(f\"Overall model output agreement rate: {overall_output_agreement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement by paper\n",
    "paper_output_agreement = output_agreement_df.groupby('paper_id')['models_agree'].mean() * 100\n",
    "paper_output_agreement = paper_output_agreement.reset_index()\n",
    "paper_output_agreement.columns = ['Paper ID', 'Model Output Agreement Rate (%)']\n",
    "paper_output_agreement.sort_values('Model Output Agreement Rate (%)', ascending=False, inplace=True)\n",
    "\n",
    "print(\"Top 5 papers by model output agreement rate:\")\n",
    "paper_output_agreement.head()\n",
    "\n",
    "print(\"\\nBottom 5 papers by model output agreement rate:\")\n",
    "paper_output_agreement.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate paper-level agreement rates\n",
    "paper_agreement = summary_df.groupby(['paper_id', 'config'])['agreement_rate'].mean().reset_index()\n",
    "paper_agreement_pivot = paper_agreement.pivot(index='paper_id', columns='config', values='agreement_rate').reset_index()\n",
    "paper_agreement_pivot.columns = ['paper_id', 'claude_openai_agreement', 'openai_claude_agreement']\n",
    "\n",
    "# Merge with model output agreement data\n",
    "merged_agreement = pd.merge(paper_agreement_pivot, paper_output_agreement, left_on='paper_id', right_on='Paper ID', how='inner')\n",
    "\n",
    "# Calculate correlation coefficients\n",
    "corr_openai_claude = merged_agreement['openai_claude_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "corr_claude_openai = merged_agreement['claude_openai_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "\n",
    "print(f\"Correlation between OpenAI-Claude validator agreement and model output agreement: {corr_openai_claude:.4f}\")\n",
    "print(f\"Correlation between Claude-OpenAI validator agreement and model output agreement: {corr_claude_openai:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
