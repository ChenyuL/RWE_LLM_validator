{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-based Li-Paper Analysis Notebook\n",
    "\n",
    "This notebook analyzes the results of the RAG-based Li-Paper checklist validation process, comparing the OpenAI-Claude and Claude-OpenAI configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "import re\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Starting RAG-based Li-Paper analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load RAG Report Data\n",
    "\n",
    "First, we'll load the RAG report data from the `output/reports_rag` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Directory containing the RAG paper results\n",
    "base_dir = 'output/reports_rag'\n",
    "\n",
    "# Function to extract data from a report file\n",
    "def extract_report_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract paper ID\n",
    "        paper_id = data.get('paper', '').replace('.pdf', '')\n",
    "        \n",
    "        # Extract validation summary\n",
    "        summary = data.get('validation_summary', {})\n",
    "        \n",
    "        # Extract model information if available\n",
    "        model_info = data.get('model_info', {})\n",
    "        if not model_info and 'model_info' in summary:\n",
    "            model_info = summary.get('model_info', {})\n",
    "        \n",
    "        # Extract items data\n",
    "        items = data.get('items', {})\n",
    "        \n",
    "        # Determine configuration based on filename or model info\n",
    "        if 'openai_claude' in file_path or (model_info.get('extractor', '').startswith('openai') and model_info.get('validator', '').startswith('claude')):\n",
    "            config = 'openai_claude'\n",
    "        elif 'claude_openai' in file_path or (model_info.get('extractor', '').startswith('claude') and model_info.get('validator', '').startswith('openai')):\n",
    "            config = 'claude_openai'\n",
    "        else:\n",
    "            # Try to determine from model info\n",
    "            extractor = model_info.get('extractor', '')\n",
    "            validator = model_info.get('validator', '')\n",
    "            \n",
    "            if 'openai' in extractor.lower() and 'claude' in validator.lower():\n",
    "                config = 'openai_claude'\n",
    "            elif 'claude' in extractor.lower() and 'openai' in validator.lower():\n",
    "                config = 'claude_openai'\n",
    "            else:\n",
    "                config = 'unknown'\n",
    "        \n",
    "        return {\n",
    "            'paper_id': paper_id,\n",
    "            'config': config,\n",
    "            'summary': summary,\n",
    "            'model_info': model_info,\n",
    "            'items': items,\n",
    "            'file_path': file_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load all report data\n",
    "all_reports = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"Error: Directory {base_dir} does not exist\")\n",
    "else:\n",
    "    # List all files in the directory\n",
    "    print(f\"Scanning directory: {base_dir}\")\n",
    "    files = os.listdir(base_dir)\n",
    "    report_files = [f for f in files if ('report' in f) and f.endswith('.json') and '_Li-Paper' in f]\n",
    "    print(f\"Found {len(report_files)} report files\")\n",
    "    \n",
    "    # Process each report file\n",
    "    for file in report_files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        report_data = extract_report_data(file_path)\n",
    "        if report_data:\n",
    "            all_reports.append(report_data)\n",
    "    \n",
    "    print(f\"Loaded {len(all_reports)} reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Summary DataFrame\n",
    "\n",
    "Next, we'll create a DataFrame with summary information from the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a DataFrame with summary information\n",
    "summary_data = []\n",
    "\n",
    "for report in all_reports:\n",
    "    summary = report['summary']\n",
    "    summary_data.append({\n",
    "        'paper_id': report['paper_id'],\n",
    "        'config': report['config'],\n",
    "        'total_items': summary.get('total_items', 0),\n",
    "        'agree_with_extractor': summary.get('agree_with_extractor', 0),\n",
    "        'disagree_with_extractor': summary.get('disagree_with_extractor', 0),\n",
    "        'unknown': summary.get('unknown', 0),\n",
    "        'agreement_rate': summary.get('agreement_rate', 0),\n",
    "        'extractor': report['model_info'].get('extractor', ''),\n",
    "        'validator': report['model_info'].get('validator', '')\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Summary DataFrame created with shape:\", summary_df.shape)\n",
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 1: Agreement Rates by Configuration\n",
    "\n",
    "We'll analyze the agreement rates by configuration (OpenAI-Claude vs. Claude-OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate average agreement rates by configuration\n",
    "avg_agreement = summary_df.groupby('config')['agreement_rate'].agg(['mean', 'std', 'count']).reset_index()\n",
    "avg_agreement.columns = ['Configuration', 'Mean Agreement Rate (%)', 'Std Dev', 'Count']\n",
    "avg_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize agreement rates by configuration\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Bar plot\n",
    "ax = sns.barplot(x='Configuration', y='Mean Agreement Rate (%)', data=avg_agreement, palette='viridis', hue='Configuration', legend=False)\n",
    "\n",
    "# Add error bars\n",
    "for i, row in avg_agreement.iterrows():\n",
    "    ax.errorbar(i, row['Mean Agreement Rate (%)'], yerr=row['Std Dev'], color='black', capsize=10, linewidth=2)\n",
    "\n",
    "# Add value labels on top of bars\n",
    "for i, v in enumerate(avg_agreement['Mean Agreement Rate (%)']):\n",
    "    ax.text(i, v + 1, f\"{v:.2f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.title('Mean Agreement Rate by Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Configuration', fontsize=14)\n",
    "plt.ylim(0, 105)  # Set y-axis limit to accommodate error bars and labels\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_agreement_by_config.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: Distribution of Agreement Rates\n",
    "\n",
    "We'll visualize the distribution of agreement rates for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize distribution of agreement rates\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.violinplot(x='config', y='agreement_rate', data=summary_df, palette='viridis', inner='box', hue='config', legend=False)\n",
    "ax = sns.swarmplot(x='config', y='agreement_rate', data=summary_df, color='white', edgecolor='black', size=8, alpha=0.7)\n",
    "\n",
    "plt.title('Distribution of Agreement Rates by Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Configuration', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_agreement_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3: Statistical Comparison of Configurations\n",
    "\n",
    "We'll perform a statistical test to compare the agreement rates between the two configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform Mann-Whitney U test (non-parametric test for independent samples)\n",
    "openai_claude = summary_df[summary_df['config'] == 'openai_claude']['agreement_rate']\n",
    "claude_openai = summary_df[summary_df['config'] == 'claude_openai']['agreement_rate']\n",
    "\n",
    "u_stat, p_value = stats.mannwhitneyu(openai_claude, claude_openai, alternative='two-sided')\n",
    "\n",
    "print(f\"Mann-Whitney U test results:\")\n",
    "print(f\"U statistic: {u_stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Significant difference at Î±=0.05: {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 4: Agreement on Individual Checklist Items\n",
    "\n",
    "We'll analyze the agreement rates for individual checklist items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract item-level data\n",
    "item_data = []\n",
    "\n",
    "for report in all_reports:\n",
    "    for item_id, item in report['items'].items():\n",
    "        item_data.append({\n",
    "            'paper_id': report['paper_id'],\n",
    "            'config': report['config'],\n",
    "            'item_id': item_id,\n",
    "            'compliance': item.get('compliance', ''),\n",
    "            'correct_answer': item.get('correct_answer', ''),\n",
    "            'description': item.get('description', '')\n",
    "        })\n",
    "\n",
    "item_df = pd.DataFrame(item_data)\n",
    "print(\"Item DataFrame created with shape:\", item_df.shape)\n",
    "item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate agreement rate by checklist item\n",
    "item_agreement = item_df.groupby(['item_id', 'config'])['compliance'].apply(\n",
    "    lambda x: (x == 'agree with extractor').mean() * 100\n",
    ").reset_index()\n",
    "item_agreement.columns = ['Item ID', 'Configuration', 'Agreement Rate (%)']\n",
    "\n",
    "# Pivot the data for easier comparison\n",
    "item_agreement_pivot = item_agreement.pivot(index='Item ID', columns='Configuration', values='Agreement Rate (%)')\n",
    "item_agreement_pivot.reset_index(inplace=True)\n",
    "\n",
    "# Sort by item ID (handle mixed numeric and string IDs)\n",
    "try:\n",
    "    # Try to convert to numeric if all IDs are numeric\n",
    "    item_agreement_pivot['Item ID'] = pd.to_numeric(item_agreement_pivot['Item ID'])\n",
    "    item_agreement_pivot.sort_values('Item ID', inplace=True)\n",
    "except ValueError:\n",
    "    # If there are non-numeric IDs, sort as strings\n",
    "    print(\"Warning: Some item IDs are not numeric. Sorting as strings.\")\n",
    "    item_agreement_pivot.sort_values('Item ID', inplace=True)\n",
    "\n",
    "item_agreement_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize agreement rates by checklist item\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Reshape data for plotting\n",
    "plot_data = pd.melt(item_agreement_pivot, id_vars=['Item ID'], \n",
    "                    value_vars=['openai_claude', 'claude_openai'],\n",
    "                    var_name='Configuration', value_name='Agreement Rate (%)')\n",
    "\n",
    "# Create the grouped bar chart\n",
    "ax = sns.barplot(x='Item ID', y='Agreement Rate (%)', hue='Configuration', data=plot_data, palette='viridis')\n",
    "\n",
    "plt.title('Agreement Rate by Checklist Item and Configuration', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Checklist Item ID', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 105)\n",
    "plt.legend(title='Configuration', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_agreement_by_item.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 5: Model Output Comparison\n",
    "\n",
    "We'll compare the model outputs between the two configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract model outputs\n",
    "model_outputs = {}\n",
    "\n",
    "for report in all_reports:\n",
    "    paper_id = report['paper_id']\n",
    "    config = report['config']\n",
    "    \n",
    "    if paper_id not in model_outputs:\n",
    "        model_outputs[paper_id] = {}\n",
    "    \n",
    "    for item_id, item in report['items'].items():\n",
    "        if item_id not in model_outputs[paper_id]:\n",
    "            model_outputs[paper_id][item_id] = {}\n",
    "        \n",
    "        model_outputs[paper_id][item_id][config] = item.get('correct_answer', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to compare model outputs and determine if they agree\n",
    "def outputs_agree(output1, output2):\n",
    "    # Simple string comparison for now\n",
    "    # Could be enhanced with semantic similarity or other NLP techniques\n",
    "    if not output1 or not output2:\n",
    "        return False\n",
    "    \n",
    "    # Check for exact match\n",
    "    if output1 == output2:\n",
    "        return True\n",
    "    \n",
    "    # Check for 'unknown' or similar values\n",
    "    unknown_patterns = ['unknown', 'not enough information', 'cannot determine']\n",
    "    if any(pattern in output1.lower() for pattern in unknown_patterns) and \\\n",
    "       any(pattern in output2.lower() for pattern in unknown_patterns):\n",
    "        return True\n",
    "    \n",
    "    # Check for yes/no agreement\n",
    "    yes_patterns = ['yes', 'complies', 'compliant', 'fulfilled']\n",
    "    no_patterns = ['no', 'does not comply', 'non-compliant', 'not fulfilled']\n",
    "    \n",
    "    output1_yes = any(pattern in output1.lower() for pattern in yes_patterns)\n",
    "    output1_no = any(pattern in output1.lower() for pattern in no_patterns)\n",
    "    output2_yes = any(pattern in output2.lower() for pattern in yes_patterns)\n",
    "    output2_no = any(pattern in output2.lower() for pattern in no_patterns)\n",
    "    \n",
    "    if (output1_yes and output2_yes) or (output1_no and output2_no):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Calculate model output agreement\n",
    "output_agreement_data = []\n",
    "\n",
    "for paper_id, items in model_outputs.items():\n",
    "    for item_id, configs in items.items():\n",
    "        if 'openai_claude' in configs and 'claude_openai' in configs:\n",
    "            openai_claude_output = configs['openai_claude']\n",
    "            claude_openai_output = configs['claude_openai']\n",
    "            \n",
    "            agreement = outputs_agree(openai_claude_output, claude_openai_output)\n",
    "            \n",
    "            output_agreement_data.append({\n",
    "                'paper_id': paper_id,\n",
    "                'item_id': item_id,\n",
    "                'openai_claude_output': openai_claude_output,\n",
    "                'claude_openai_output': claude_openai_output,\n",
    "                'models_agree': agreement\n",
    "            })\n",
    "\n",
    "output_agreement_df = pd.DataFrame(output_agreement_data)\n",
    "print(\"Output agreement DataFrame created with shape:\", output_agreement_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate overall model output agreement rate\n",
    "overall_output_agreement = output_agreement_df['models_agree'].mean() * 100\n",
    "print(f\"Overall model output agreement rate: {overall_output_agreement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement by paper\n",
    "paper_output_agreement = output_agreement_df.groupby('paper_id')['models_agree'].mean() * 100\n",
    "paper_output_agreement = paper_output_agreement.reset_index()\n",
    "paper_output_agreement.columns = ['Paper ID', 'Model Output Agreement Rate (%)']\n",
    "paper_output_agreement.sort_values('Model Output Agreement Rate (%)', ascending=False, inplace=True)\n",
    "\n",
    "print(\"Top 5 papers by model output agreement rate:\")\n",
    "paper_output_agreement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Bottom 5 papers by model output agreement rate:\")\n",
    "paper_output_agreement.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model output agreement by paper\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x='Paper ID', y='Model Output Agreement Rate (%)', data=paper_output_agreement, palette='viridis', hue='Paper ID', legend=False)\n",
    "\n",
    "plt.title('Model Output Agreement Rate by Paper', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Paper ID', fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_model_output_agreement_by_paper.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate model output agreement by checklist item\n",
    "item_output_agreement = output_agreement_df.groupby('item_id')['models_agree'].mean() * 100\n",
    "item_output_agreement = item_output_agreement.reset_index()\n",
    "item_output_agreement.columns = ['Item ID', 'Model Output Agreement Rate (%)']\n",
    "\n",
    "# Sort by item ID (handle mixed numeric and string IDs)\n",
    "try:\n",
    "    # Try to convert to numeric if all IDs are numeric\n",
    "    item_output_agreement['Item ID'] = pd.to_numeric(item_output_agreement['Item ID'])\n",
    "    item_output_agreement.sort_values('Item ID', inplace=True)\n",
    "except ValueError:\n",
    "    # If there are non-numeric IDs, sort as strings\n",
    "    print(\"Warning: Some item IDs are not numeric. Sorting as strings.\")\n",
    "    item_output_agreement.sort_values('Item ID', inplace=True)\n",
    "\n",
    "item_output_agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize model output agreement by checklist item\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x='Item ID', y='Model Output Agreement Rate (%)', data=item_output_agreement, palette='viridis', hue='Item ID', legend=False)\n",
    "\n",
    "plt.title('Model Output Agreement Rate by Checklist Item', fontsize=16)\n",
    "plt.ylabel('Agreement Rate (%)', fontsize=14)\n",
    "plt.xlabel('Checklist Item ID', fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim(0, 105)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig('rag_model_output_agreement_by_item.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 6: Correlation Between Validator Agreement and Model Output Agreement\n",
    "\n",
    "We'll examine the correlation between validator agreement and model output agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate paper-level agreement rates\n",
    "paper_agreement = summary_df.groupby(['paper_id', 'config'])['agreement_rate'].mean().reset_index()\n",
    "paper_agreement_pivot = paper_agreement.pivot(index='paper_id', columns='config', values='agreement_rate').reset_index()\n",
    "paper_agreement_pivot.columns = ['paper_id', 'claude_openai_agreement', 'openai_claude_agreement']\n",
    "\n",
    "# Merge with model output agreement data\n",
    "merged_agreement = pd.merge(paper_agreement_pivot, paper_output_agreement, left_on='paper_id', right_on='Paper ID', how='inner')\n",
    "print(\"Merged agreement DataFrame created with shape:\", merged_agreement.shape)\n",
    "merged_agreement.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate correlation coefficients\n",
    "corr_openai_claude = merged_agreement['openai_claude_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "corr_claude_openai = merged_agreement['claude_openai_agreement'].corr(merged_agreement['Model Output Agreement Rate (%)'])\n",
    "\n",
    "print(f\"Correlation between OpenAI-Claude validator agreement and model output agreement: {corr_openai_claude:.4f}\")\n",
    "print(f\"Correlation between Claude-OpenAI validator agreement and model output agreement: {corr_claude_openai:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create scatter plots to visualize correlations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot for OpenAI-Claude configuration\n",
    "sns.regplot(x='openai_claude_agreement', y='Model Output Agreement Rate (%)', \n",
    "            data=merged_agreement, ax=axes[0], scatter_kws={'alpha':0.7, 's':100}, line_kws={'color':'red'})\n",
    "axes[0].set_title(f'OpenAI-Claude Validator Agreement vs. Model Output Agreement\\nCorrelation: {corr_openai_claude:.4f}', fontsize=14)\n",
    "axes[0].set_xlabel('OpenAI-Claude Validator Agreement Rate (%)', fontsize=12)\n",
    "axes[0].set_ylabel('Model Output Agreement Rate (%)', fontsize=12)\n",
    "axes[0].grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Plot for Claude-OpenAI configuration\n",
    "sns.regplot(x='claude_openai_agreement', y='Model Output Agreement Rate (%)', \n",
    "            data=merged_agreement, ax=axes[1], scatter_kws={'alpha':0.7, 's':100}, line_kws={'color':'red'})\n",
    "axes[1].set_title(f'Claude-OpenAI Validator Agreement vs. Model Output Agreement\\nCorrelation: {corr_claude_openai:.4f}', fontsize=14)\n",
    "axes[1].set_xlabel('Claude-OpenAI Validator Agreement Rate (%)', fontsize=12)\n",
    "axes[1].set_ylabel('Model Output Agreement Rate (%)', fontsize=
